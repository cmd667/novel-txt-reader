import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import os
import re

def get_page_text(driver, curr_url):
    """
    Purpose: è·å–æœ¬é¡µå°è¯´å†…å®¹
    """
    driver.get(curr_url)
    nr1_element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.ID, "nr1"))
    )
    html = nr1_element.get_attribute("innerHTML")
    # åˆ é™¤ HTML åŸæœ¬çš„æ¢è¡Œç¬¦
    html = html.replace("\n", "")

    soup = BeautifulSoup(html, 'html.parser')

    # å»æ‰åˆ†é¡µåŒºå—å’Œæç¤ºå†…å®¹
    for tag in soup.select("center.chapterPages, font, br+center"):
        tag.decompose()    
    
    # å°†æ‰€æœ‰ <br> æ›¿æ¢æˆæ¢è¡Œ
    for br in soup.find_all("br"):
        br.replace_with("\n")

    # æå–çº¯æ–‡æœ¬
    raw_text = soup.get_text()
    # with open("raw.txt",'w',encoding='utf-8') as f:
    #     f.write(raw_text)
    # æ ¹æ®è§„åˆ™ï¼šåˆ é™¤æ¢è¡Œç¬¦åé¢ç´§è·Ÿæ–‡å­—çš„æƒ…å†µ
    cleaned_text = re.sub(r'\n(?=\S)', '', raw_text)
    cleaned_text = re.sub(r'\n(?=\S)', '', cleaned_text)
    # with open("cl.txt",'w',encoding='utf-8') as f:
    #     f.write(cleaned_text.strip())
    # exit()
    return cleaned_text.strip()
# end def

def get_chapter_text(driver, chapter_url):
    """
    Purpose:æ•´åˆå°è¯´åˆ†é¡µå†…å®¹ï¼Œå•ç« ä¿å­˜ 
    """
    print("æ­£åœ¨åŠ è½½ç« èŠ‚...")
    driver.get(chapter_url)
    # time.sleep(1)
    # è·å–ç« èŠ‚å
    chapter_title = driver.find_element(By.CSS_SELECTOR, "h1.page-title").text

    chapter = []
    page_count = 1
    urls = []
    
    # æŸ¥æ‰¾æ‰€æœ‰åˆ†é¡µé“¾æ¥ï¼Œå¡«å…¥åç»­é¡µçš„æ–‡æœ¬
    page_links = driver.find_elements(By.CSS_SELECTOR, "center.chapterPages a")

    # ç”±äºåç»­ä¼šè·³è½¬é¡µé¢ï¼Œæ­¤å¤„ç”Ÿæˆurlsæ•°ç»„å¤‡ç”¨
    urls.append(chapter_url)
    for page_link in page_links:
        urls.append(page_link.get_attribute("href"))
    for url in urls:
        chapter.append(get_page_text(driver, url)) # è·å–å†…å®¹
        print(f"è·å–ç« èŠ‚ {chapter_title} é¡µæ•°:{page_count}")
        page_count += 1
    return chapter
# end def

def get_novel_by_catalog(driver, catalog_url):
    """
    Purpose: æ ¹æ®ç›®å½•è·å–æ‰€æœ‰ç« èŠ‚
    """
    print("æ­£åœ¨åŠ è½½ç›®å½•...")
    driver.get(catalog_url)
    time.sleep(2)
    # driver.save_screenshot('diyibanzhu.png')
    # è·å–å°è¯´æ ‡é¢˜ï¼ˆåœ¨ <h1> ä¸­ï¼‰
    try:
        novel_title = driver.find_element(By.CSS_SELECTOR, "div.right h1").text 
        print("è·å–å°è¯´ï¼š" + novel_title)
    except Exception as e:
        print("âŒ æ— æ³•è·å–å°è¯´æ ‡é¢˜:", e)
        return    
    # åˆ›å»ºä»¥å°è¯´æ ‡é¢˜ä¸ºåçš„æ–‡ä»¶å¤¹
    folder_path = os.path.join("novels", novel_title)
    os.makedirs(folder_path, exist_ok=True)
    print(f"ğŸ“ å°è¯´ç›®å½•å·²åˆ›å»ºï¼š{folder_path}")
    # ä»ç›®å½•è·å–å…¨éƒ¨ç›®å½•åˆ†é¡µ
    # å®šä½selectæ ‡ç­¾
    select_element = driver.find_element(By.CSS_SELECTOR, 'select[name="pagelist"]')
    # æ‰¾åˆ°æ‰€æœ‰optionæ ‡ç­¾ï¼Œæå–valueå±æ€§
    option_elements = select_element.find_elements(By.TAG_NAME, "option")
    option_links = [option.get_attribute("value") for option in option_elements]

    index = 1
    for link in option_links:
        driver.get(f"https://m.diyibanzhu.space/{link}")
        # ç« èŠ‚é“¾æ¥ä½äº ul.list li a ä¸­
        chapter_list_div = driver.find_elements(By.CSS_SELECTOR, "div.mod.block.update.chapter-list")
        if len(chapter_list_div) == 2:
            chapter_links = chapter_list_div[1].find_elements(By.CSS_SELECTOR, "ul.list li a")
        else:
            chapter_links = chapter_list_div[0].find_elements(By.CSS_SELECTOR, "ul.list li a")
        
        # æŒ‰ç…§æ‰€æœ‰ç›®å½•åˆ†é¡µï¼Œåˆ¶ä½œchapter_urls
        chapter_urls = []
        for chapter_link in chapter_links:
            chapter_urls.append(chapter_link.get_attribute("href"))
        # è¯»å–æ¯ç« å†…å®¹
        for chapter_url in chapter_urls:
            chapter_text = get_chapter_text(driver, chapter_url)
            chapter_title = driver.find_element(By.CSS_SELECTOR, "div.container h1").text
            safe_title = re.sub(r'[\\/:*?"<>|]', '', chapter_title).strip() 
            filename = os.path.join(folder_path, f"{index:04d}_{safe_title}.txt")
            with open(filename, 'w', encoding='UTF-8') as f:
                for page_text in chapter_text:
                    f.write(page_text)
            print(f"âœ… å·²ä¿å­˜ç« èŠ‚ï¼š{filename}")
            f.close()
            index += 1
# end def

if __name__ == "__main__":

    # chapter_url = 'https://m.diyibanzhu.space/view/777076.html'
    # catalog_url = 'https://m.diyibanzhu.space/list/12170.html'

    options = uc.ChromeOptions()
    options.add_argument('--headless') # æ— å¤´æ¨¡å¼
    options.add_argument('lang=zh_CN.UTF-8') # ä¸­æ–‡
    options.add_argument('--disable-gpu') # ç¦ç”¨gpuåŠ é€Ÿ
    options.add_argument('--blink-settings=imagesEnabled=False') # ç¦æ­¢åŠ è½½å›¾ç‰‡ï¼Œé™ä½æ€§èƒ½éœ€æ±‚

    driver = uc.Chrome(options=options)
    
    try:
        while True:
            novel_id = input("è¯·è¾“å…¥å°è¯´ç›®å½•idï¼ˆç›´æ¥å›è½¦é€€å‡ºï¼‰ï¼š").strip()
            if not novel_id:
                print("é€€å‡ºç¨‹åº")
                break
            catalog_url = f'https://m.diyibanzhu.space/list/{novel_id}.html'
            get_novel_by_catalog(driver, catalog_url)
    except KeyboardInterrupt:
        print("\næ‰‹åŠ¨ä¸­æ–­ç¨‹åº")
    finally:
        driver.quit()